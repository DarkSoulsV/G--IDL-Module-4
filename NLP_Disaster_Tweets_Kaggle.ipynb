{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6d89907-9afa-452a-8f86-7610da6a9d2d",
   "metadata": {},
   "source": [
    "# NLP Disaster Tweets — Mini Project (Week 4)\n",
    "\n",
    "This project is part of the Deep Learning Module 4 assignment.  \n",
    "The goal is to classify tweets as **disaster-related (1)** or **not disaster-related (0)** using NLP techniques.\n",
    "\n",
    "Dataset source:  \n",
    "Kaggle — “NLP Getting Started” competition  \n",
    "https://www.kaggle.com/competitions/nlp-getting-started\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05cbd24-9934-4ca5-944d-c5f1131f5d9f",
   "metadata": {},
   "source": [
    "## Alexander Voit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7407e6ce-0a1f-4286-8376-ceb0f3c42d52",
   "metadata": {},
   "source": [
    "## 1. Problem Description & Data Overview\n",
    "\n",
    "This is a binary classification task where the goal is to determine whether a tweet is related to a real disaster (label = 1) or not (label = 0).  \n",
    "The dataset contains short, noisy Twitter messages that may include hashtags, links, emojis, abbreviations, and informal language — making it a typical real-world NLP problem.\n",
    "\n",
    "**Dataset summary:**\n",
    "- Training samples: 7,613 tweets  \n",
    "- Test samples: 3,263 tweets  \n",
    "- Columns:\n",
    "  - `id`  \n",
    "  - `keyword` (may contain helpful context)  \n",
    "  - `location` (optional, often noisy)  \n",
    "  - `text` (main feature: the tweet itself)  \n",
    "  - `target` (0 or 1, only in training set)\n",
    "\n",
    "Next, we load the dataset and inspect its structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e9eaf0-26d1-4e64-9aad-92c2c9c10a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "sample_sub = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "# Show basic info\n",
    "train.head(), train.shape, test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140b61ab-ef92-489b-a4ea-c42eb37c8913",
   "metadata": {},
   "source": [
    "**Observation:**  \n",
    "The dataset contains 7,613 rows and 5 columns.  \n",
    "`keyword` and `location` have missing values, which is expected for tweets and will not prevent modeling since the main signal is in the `text` column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9907ed6-7d72-49db-8a14-850c6a9f7131",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)\n",
    "\n",
    "In this step, we inspect the dataset to understand its structure, identify missing values,  \n",
    "explore the distribution of the target variable, and analyze tweet characteristics such as text length.\n",
    "\n",
    "The goal of this EDA is to decide how to clean and preprocess the tweets before modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83976650-f485-43d2-ad50-295619fb39cf",
   "metadata": {},
   "source": [
    "### 2.1 Data Structure Overview\n",
    "\n",
    "We begin by inspecting the dataset structure, column types, and the presence of missing values.  \n",
    "This helps us understand how clean the data is and what preprocessing will be necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9582e6-2710-49af-9d38-c2892bd85fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information\n",
    "train.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd955b1-99cb-42d0-b4c7-bf6bbfda65c7",
   "metadata": {},
   "source": [
    "**Observation:**  \n",
    "The classes are slightly imbalanced: non-disaster tweets are more frequent.  \n",
    "However, the imbalance is not severe and we can proceed without applying special balancing techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c872f8-85b7-4c6d-823f-fc72615d4827",
   "metadata": {},
   "source": [
    "### 2.2 Target Distribution\n",
    "\n",
    "Next, we examine the balance between the two classes:  \n",
    "- `0` → non-disaster  \n",
    "- `1` → disaster  \n",
    "\n",
    "This helps evaluate whether we need to address class imbalance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae88410-9781-40a4-9518-4498e25a3ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.countplot(data=train, x=\"target\")\n",
    "plt.title(\"Target Distribution (0 = non-disaster, 1 = disaster)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4659a6-88a9-4e02-a1cd-ab598358d36c",
   "metadata": {},
   "source": [
    "**Observation:**  \n",
    "Most tweets have between 50 and 150 characters.  \n",
    "This indicates that they are short, noisy, and require cleaning but not heavy truncation.  \n",
    "We will use full tweets as input for the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00c6e17-d505-4862-b09f-5209fff65b67",
   "metadata": {},
   "source": [
    "### 2.3 Tweet Length Distribution\n",
    "\n",
    "We analyze the number of characters per tweet to understand the typical tweet size.  \n",
    "This helps determine preprocessing limits such as maximum sequence length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b28870d-a9e4-46f1-bae6-05e8e9c0bcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"text_len\"] = train[\"text\"].apply(len)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(train[\"text_len\"], bins=40, kde=True)\n",
    "plt.title(\"Tweet Length Distribution\")\n",
    "plt.xlabel(\"Tweet length (characters)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "train[\"text_len\"].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f726512-1680-471b-a374-0c1c1e368cde",
   "metadata": {},
   "source": [
    "**Observation:**  \n",
    "Most tweets fall between 50 and 150 characters.  \n",
    "The distribution is typical for Twitter data: short, noisy messages.  \n",
    "We will keep full text length during preprocessing since truncation is not necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886b8349-0dc3-4083-ab8b-8449681d2fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0e38c7-967e-4f85-befc-ea314747db39",
   "metadata": {},
   "source": [
    "**Observation:**  \n",
    "`keyword` and `location` contain missing values, but these fields are optional and not critical.  \n",
    "Our main feature is the tweet text, so we can proceed without filling these columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29c19e3-c157-474f-886c-b239444e2bd2",
   "metadata": {},
   "source": [
    "## 3. Model Architecture\n",
    "\n",
    "In this section, we describe the preprocessing pipeline, feature extraction methods,  \n",
    "and the model architectures used for the classification task.  \n",
    "We begin with traditional NLP approaches (TF-IDF + Logistic Regression)  \n",
    "and then build a deep learning model using an Embedding layer followed by an LSTM/GRU network.\n",
    "\n",
    "The goal is not only to build a working model but also to demonstrate understanding  \n",
    "of how sequential neural networks (RNN,LSTM,GRU) process text data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7291cf1f-3ce5-45c2-895a-ba9ea5b7e188",
   "metadata": {},
   "source": [
    "### 3.1 Text Preprocessing\n",
    "\n",
    "Tweets often contain URLs, punctuation, mentions, hashtags, and inconsistent casing.  \n",
    "To clean the text before tokenization, we apply the following steps:\n",
    "\n",
    "- Lowercase all text  \n",
    "- Remove URLs  \n",
    "- Remove HTML tags  \n",
    "- Remove punctuation  \n",
    "- Remove numbers  \n",
    "- Remove extra spaces  \n",
    "- (Optional) Remove stopwords  \n",
    "\n",
    "We implement a reusable `clean_text()` function for both training and test datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515adb97-a6cb-4ab6-9382-a951bf0c7878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "    # remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "train[\"clean_text\"] = train[\"text\"].apply(clean_text)\n",
    "test[\"clean_text\"] = test[\"text\"].apply(clean_text)\n",
    "\n",
    "train[[\"text\", \"clean_text\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0b737a-c8cb-4396-930c-0e8812918f02",
   "metadata": {},
   "source": [
    "**Observation:**  \n",
    "After preprocessing, the tweets become cleaner and more uniform.  \n",
    "URLs, punctuation, and noise are removed, which helps the model focus on semantic content  \n",
    "rather than surface-level artifacts. This prepares the text for vectorization and embedding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120d6228-f629-402b-a421-182847a9af70",
   "metadata": {},
   "source": [
    "### 3.2 Feature Extraction\n",
    "\n",
    "We use two feature extraction methods:\n",
    "\n",
    "**1. TF-IDF (traditional NLP approach)**  \n",
    "This converts each tweet into a sparse vector based on term frequency and inverse document frequency.  \n",
    "It serves as a strong baseline for text classification with simple models such as Logistic Regression.\n",
    "\n",
    "**2. Tokenizer + Embedding (for RNN models)**  \n",
    "For LSTM/GRU, we convert text into integer sequences and learn dense word embeddings.  \n",
    "This allows the model to capture semantic relationships between words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9f6dff-7dc3-4a8d-b148-c0f6805d614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "\n",
    "X_tfidf = tfidf.fit_transform(train[\"clean_text\"])\n",
    "y = train[\"target\"]\n",
    "\n",
    "# Train/validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_tfidf, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Baseline model: Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=200)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = log_reg.predict(X_val)\n",
    "\n",
    "# Accuracy\n",
    "baseline_acc = accuracy_score(y_val, y_pred)\n",
    "baseline_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657f33b7-7219-4485-bf55-585621d1bc2f",
   "metadata": {},
   "source": [
    "**Observation:**  \n",
    "The baseline TF-IDF + Logistic Regression model achieved an accuracy of **0.8056** on the validation set.  \n",
    "This is a strong classical NLP baseline and shows that simple bag-of-words features already capture useful\n",
    "signal in the dataset.  \n",
    "We will now build a deep learning model (LSTM/GRU) to determine whether sequential modeling can improve performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8cb129-caaf-4a66-9bfc-8c9e60912d6e",
   "metadata": {},
   "source": [
    "### 3.3 Tokenization and Sequence Preparation (for LSTM/GRU)\n",
    "\n",
    "For the deep learning model, we convert each cleaned tweet into a sequence of integer indices:\n",
    "\n",
    "1. Fit a Keras `Tokenizer` on the cleaned training text.  \n",
    "2. Transform tweets into integer sequences.  \n",
    "3. Pad or truncate sequences to a fixed maximum length.\n",
    "\n",
    "This representation allows us to use an Embedding layer followed by an LSTM/GRU network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff5da72-201f-47f9-9278-71c59eb776ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "max_words = 20000      # maximum vocabulary size\n",
    "max_len = 40           # maximum tweet length in tokens (we'll experiment later)\n",
    "\n",
    "# Fit tokenizer on training text\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train[\"clean_text\"].values)\n",
    "\n",
    "# Text -> sequences\n",
    "X_seq = tokenizer.texts_to_sequences(train[\"clean_text\"].values)\n",
    "X_test_seq = tokenizer.texts_to_sequences(test[\"clean_text\"].values)\n",
    "\n",
    "# Pad sequences\n",
    "X_pad = pad_sequences(X_seq, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "y = train[\"target\"].values\n",
    "\n",
    "X_pad.shape, X_test_pad.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2036d7-5d38-43ce-9603-5f0ef6819630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
