{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6d89907-9afa-452a-8f86-7610da6a9d2d",
   "metadata": {},
   "source": [
    "# üß† NLP Disaster Tweets ‚Äî Mini Project (Week 4)\n",
    "\n",
    "This project is part of the Deep Learning Module 4 assignment.  \n",
    "The goal is to classify tweets as **disaster-related (1)** or **not disaster-related (0)** using NLP techniques.\n",
    "\n",
    "Dataset source:  \n",
    "Kaggle ‚Äî ‚ÄúNLP Getting Started‚Äù competition  \n",
    "https://www.kaggle.com/competitions/nlp-getting-started\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05cbd24-9934-4ca5-944d-c5f1131f5d9f",
   "metadata": {},
   "source": [
    "## Alexander Voit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7407e6ce-0a1f-4286-8376-ceb0f3c42d52",
   "metadata": {},
   "source": [
    "## 1. Problem Description & Data Overview\n",
    "\n",
    "This is a binary classification task where the goal is to determine whether a tweet is related to a real disaster (label = 1) or not (label = 0).  \n",
    "The dataset contains short, noisy Twitter messages that may include hashtags, links, emojis, abbreviations, and informal language ‚Äî making it a typical real-world NLP problem.\n",
    "\n",
    "**Dataset summary:**\n",
    "- Training samples: 7,613 tweets  \n",
    "- Test samples: 3,263 tweets  \n",
    "- Columns:\n",
    "  - `id`  \n",
    "  - `keyword` (may contain helpful context)  \n",
    "  - `location` (optional, often noisy)  \n",
    "  - `text` (main feature: the tweet itself)  \n",
    "  - `target` (0 or 1, only in training set)\n",
    "\n",
    "Next, we load the dataset and inspect its structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e9eaf0-26d1-4e64-9aad-92c2c9c10a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "sample_sub = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "# Show basic info\n",
    "train.head(), train.shape, test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140b61ab-ef92-489b-a4ea-c42eb37c8913",
   "metadata": {},
   "source": [
    "**Observation:**  \n",
    "The dataset contains 7,613 rows and 5 columns.  \n",
    "`keyword` and `location` have missing values, which is expected for tweets and will not prevent modeling since the main signal is in the `text` column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9907ed6-7d72-49db-8a14-850c6a9f7131",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)\n",
    "\n",
    "In this step, we inspect the dataset to understand its structure, identify missing values,  \n",
    "explore the distribution of the target variable, and analyze tweet characteristics such as text length.\n",
    "\n",
    "The goal of this EDA is to decide how to clean and preprocess the tweets before modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83976650-f485-43d2-ad50-295619fb39cf",
   "metadata": {},
   "source": [
    "### 2.1 Data Structure Overview\n",
    "\n",
    "We begin by inspecting the dataset structure, column types, and the presence of missing values.  \n",
    "This helps us understand how clean the data is and what preprocessing will be necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9582e6-2710-49af-9d38-c2892bd85fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information\n",
    "train.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd955b1-99cb-42d0-b4c7-bf6bbfda65c7",
   "metadata": {},
   "source": [
    "**Observation:**  \n",
    "The classes are slightly imbalanced: non-disaster tweets are more frequent.  \n",
    "However, the imbalance is not severe and we can proceed without applying special balancing techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c872f8-85b7-4c6d-823f-fc72615d4827",
   "metadata": {},
   "source": [
    "### 2.2 Target Distribution\n",
    "\n",
    "Next, we examine the balance between the two classes:  \n",
    "- `0` ‚Üí non-disaster  \n",
    "- `1` ‚Üí disaster  \n",
    "\n",
    "This helps evaluate whether we need to address class imbalance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae88410-9781-40a4-9518-4498e25a3ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.countplot(data=train, x=\"target\")\n",
    "plt.title(\"Target Distribution (0 = non-disaster, 1 = disaster)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4659a6-88a9-4e02-a1cd-ab598358d36c",
   "metadata": {},
   "source": [
    "**Observation:**  \n",
    "Most tweets have between 50 and 150 characters.  \n",
    "This indicates that they are short, noisy, and require cleaning but not heavy truncation.  \n",
    "We will use full tweets as input for the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00c6e17-d505-4862-b09f-5209fff65b67",
   "metadata": {},
   "source": [
    "### 2.3 Tweet Length Distribution\n",
    "\n",
    "We analyze the number of characters per tweet to understand the typical tweet size.  \n",
    "This helps determine preprocessing limits such as maximum sequence length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b28870d-a9e4-46f1-bae6-05e8e9c0bcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"text_len\"] = train[\"text\"].apply(len)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(train[\"text_len\"], bins=40, kde=True)\n",
    "plt.title(\"Tweet Length Distribution\")\n",
    "plt.xlabel(\"Tweet length (characters)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "train[\"text_len\"].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f726512-1680-471b-a374-0c1c1e368cde",
   "metadata": {},
   "source": [
    "**Observation:**  \n",
    "Most tweets fall between 50 and 150 characters.  \n",
    "The distribution is typical for Twitter data: short, noisy messages.  \n",
    "We will keep full text length during preprocessing since truncation is not necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886b8349-0dc3-4083-ab8b-8449681d2fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0e38c7-967e-4f85-befc-ea314747db39",
   "metadata": {},
   "source": [
    "**Observation:**  \n",
    "`keyword` and `location` contain missing values, but these fields are optional and not critical.  \n",
    "Our main feature is the tweet text, so we can proceed without filling these columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a5e14c-c41f-4d13-8f67-b4134b913f50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
